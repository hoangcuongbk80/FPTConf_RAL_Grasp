\section{Introduction}
\label{sec:intro}

Grasp configuration generation stands as a critical element in robotic manipulation, and vision-based methodologies have played a pivotal role in addressing this challenge \cite{hoang2023grasp, hoang2022context, fang2020graspnet}. While model-based grasp generation has been prevalent, its limitations become apparent, particularly when confronting unknown objects \cite{du2021vision, hoang2020panoptic, hoang2020object, hoang2016sub}. The reliance on pre-defined 3D models and grasp databases presents significant constraints in real-world scenarios where robots encounter diverse, unmodeled objects. Conventionally, model-based grasp generation methods rely on a 6D object pose estimation algorithm to align a Computer-Aided Design (CAD) model with measured data, followed by selecting grasps from a pre-computed database. However, synthesizing grasps for unknown objects becomes implausible, as these approaches presume the availability of a 3D model and a pre-defined grasp database \cite{du2021vision}.

An alternative avenue involves generating grasp configurations directly from sensor data without presuming knowledge of the object's 3D model or pre-computed grasps, referred to as grasp generation or grasp detection \cite{ten2017grasp, fang2020graspnet, hoang2022context}. Current methods fall into two categories: planar grasping and six Degrees of Freedom (6-DoF) grasping. Planar grasping utilizes a simple yet effective representation defining grasps as oriented bounding boxes. While this low degree of freedom (DoF) representation simplifies the task to a detection problem, it restricts performance in 3D manipulation tasks. On the other hand, 6-DoF grasping offers greater dexterity, suitable for complex scenarios. However, accurate generation of 6-DoF grasps necessitates geometric information, leading many existing methods to rely on 3D point cloud data. Despite significant progress achieved by grasp generation methods using point clouds, challenges persist due to measurement noise, occlusions, and environmental interference, making generating feasible and reliable grasps in cluttered scenes difficult. Additionally, many methods require time-consuming multi-stage processing for sampling grasp candidates and evaluating grasp quality, while the unavailability of 3D point cloud data in numerous applications exacerbates this issue \cite{ten2017grasp, liang2019pointnetgpd, du2021vision}. In contrast to 3D point clouds, acquiring color (RGB) images is more cost-effective and straightforward. With the advancements in deep learning, tasks such as object detection \cite{zou2023object} or 6D object pose estimation \cite{marullo20236d} from RGB images have exhibited remarkable performance. However, the domain of grasp detection from RGB images remains largely unexplored.

This study introduces a novel deep learning network for model-free 6-DoF grasping, exclusively leveraging an RGB image for accurate grasp estimation, building upon our prior work \cite{hoang2023grasp}. To derive geometric information crucial for prediction, we harness recent advancements in monocular depth estimation to extract 3D points. To the best of our knowledge, this is the first deep learning pipeline using 3D point clouds from estimated depth maps for grasp generation. Given an RGB image and a predicted 3D point cloud, we propose an attention-based adaptive fusion module to extract discriminative features. These features are subsequently fed into a deep Hough voting module, inspired by our prior work demonstrating the efficacy of voting mechanisms in addressing occlusions and ensuring collision-free grasps. The voting module, built upon our prior research, enables the identification of optimal grasp centers. Subsequently, the collected votes undergo clustering and regression processes to precisely determine the essential grasp parameters. We evaluate the proposed method on a standard dataset and on real robot grasping application. The results show that even using only RGB images with estimated depth maps we still outperform state-of-the-art methods using depth images from sensors. Our method's evaluation on a standard dataset and real robot grasping applications underscores its superiority, showcasing that even with solely RGB images and estimated depth maps, our approach outperforms state-of-the-art methods utilizing depth images from sensors.

The remainder of this article is organized as follows: Section II, Literature Review, provides an overview of Learning-based Grasp Generation (II.A) and Monocular Depth Estimation (II.B). In Section III, Materials and Methods, we introduce our methodology. This includes Depth Estimation (III.A) and details our innovations: the Attention-based Adaptive Fusion Network (III.B) incorporating Visual-Guided 3D Geometric Feature Learning and Geometric-Guided Visual Feature Learning, as well as the Voting-based Grasp Generation (III.C) approach. Additionally, this section covers information about the Dataset (III.D). Section IV, Evaluation, includes the Implementation Details subsection (IV.A), discussing the technical specifics of our methods. The Evaluation on GraspNet-1Billion (IV.B) subsection presents the results and analysis based on the GraspNet-1Billion dataset. Finally, the Robotic Grasping Experiment (IV.C) subsection provides insights derived from real-world experiments in robotic grasping. Lastly, Section V, Conclusions, offers a summary of the key contributions and findings presented in this article. It also outlines potential avenues for future research, paving the way for further advancements in this domain.