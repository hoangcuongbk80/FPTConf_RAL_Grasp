\section{Introduction}
\label{sec:intro}

Grasp configuration generation stands as a critical element in robotic manipulation, and vision-based methodologies have played a pivotal role in addressing this challenge \cite{hoang2023grasp, hoang2022context, fang2020graspnet}. While model-based grasp generation has been prevalent, its limitations become apparent, particularly when confronting unknown objects \cite{du2021vision, hoang2020panoptic, hoang2020object, hoang2016sub}. An alternative avenue involves generating grasp configurations directly from sensor data without presuming knowledge of the object's 3D model or pre-computed grasps, referred to as grasp generation or grasp detection \cite{fang2020graspnet, hoang2022context}. Current methods fall into two categories: planar grasping and six Degrees of Freedom (6-DoF) grasping. Planar grasping utilizes a simple yet effective representation defining grasps as oriented bounding boxes. While this low degree of freedom (DoF) representation simplifies the task to a detection problem, it restricts performance in 3D manipulation tasks. On the other hand, 6-DoF grasping offers greater dexterity, suitable for complex scenarios. However, accurate generation of 6-DoF grasps necessitates geometric information, leading many existing methods to rely on 3D point cloud data. Despite significant progress achieved by grasp generation methods using point clouds, challenges persist due to measurement noise, occlusions, and environmental interference, making generating feasible and reliable grasps in cluttered scenes difficult. Additionally, many methods require time-consuming multi-stage processing for sampling grasp candidates and evaluating grasp quality, while the unavailability of 3D point cloud data in numerous applications exacerbates this issue \cite{ten2017grasp, liang2019pointnetgpd, du2021vision}. In contrast to 3D point clouds, acquiring color (RGB) images is more cost-effective and straightforward. With the advancements in deep learning, tasks such as object detection or 6D object pose estimation from RGB images have exhibited remarkable performance. However, the domain of grasp detection from RGB images remains largely unexplored.

In this work, we present a deep learning approach for grasp generation, focusing exclusively on leveraging RGB images to achieve accurate grasp estimation, building upon our previous research efforts \cite{hoang2023grasp, hoang2022context}. To obtain crucial geometric information for prediction, we employ recent advancements in monocular depth estimation to extract 3D points. Using an RGB image and the predicted 3D point cloud, we introduce an adaptive fusion module to extract discriminative features. These features are then input into a deep Hough voting module, inspired by our prior works \cite{hoang2023grasp, hoang2022context}, which has demonstrated the effectiveness of voting mechanisms in addressing occlusions and ensuring collision-free grasps. Following the voting module, the collected votes undergo clustering and regression processes to precisely determine the essential grasp parameters. We evaluate the proposed method on a standard dataset and in a real-world robot grasping application. The results demonstrate promising outcomes, indicating that even with the utilization of only RGB images and estimated depth maps, we achieve noteworthy results.

The remainder of this article is organized as follows: Section II, Literature Review, provides an overview of Learning-based Grasp Generation (II.A) and Monocular Depth Estimation (II.B). In Section III, Materials and Methods, we introduce our methodology. This includes Depth Estimation (III.A) and details our innovations: the Attention-based Adaptive Fusion Network (III.B) incorporating Visual-Guided 3D Geometric Feature Learning and Geometric-Guided Visual Feature Learning, as well as the Voting-based Grasp Generation (III.C) approach. Additionally, this section covers information about the Dataset (III.D). Section IV, Evaluation, includes the Implementation Details subsection (IV.A), discussing the technical specifics of our methods. The Evaluation on GraspNet-1Billion (IV.B) subsection presents the results and analysis based on the GraspNet-1Billion dataset. Finally, the Robotic Grasping Experiment (IV.C) subsection provides insights derived from real-world experiments in robotic grasping. Lastly, Section V, Conclusions, offers a summary of the key contributions and findings presented in this article. It also outlines potential avenues for future research, paving the way for further advancements in this domain.