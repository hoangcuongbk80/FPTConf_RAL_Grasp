\section{Introduction}
\label{sec:intro}

Grasp configuration generation stands as a critical element in robotic manipulation, and vision-based methodologies have played a pivotal role in addressing this challenge \cite{hoang2023grasp, hoang2022context, fang2020graspnet}. While model-based grasp generation has been prevalent, its limitations become apparent, particularly when confronting unknown objects \cite{du2021vision, hoang2020panoptic, hoang2020object, hoang2019object, hoang2016sub, hoang2022voting, vu2024occlusion, palleschi2020fully, chen2016innovative}. An alternative avenue involves generating grasp configurations directly from sensor data without presuming knowledge of the object's 3D model or pre-computed grasps, referred to as grasp generation or grasp detection \cite{fang2020graspnet, hoang2022context}. Current methods fall into two categories: planar grasping and six Degrees of Freedom (6-DoF) grasping. Planar grasping utilizes a simple yet effective representation defining grasps as oriented bounding boxes. While this low degree of freedom (DoF) representation simplifies the task to a detection problem, it restricts performance in 3D manipulation tasks. On the other hand, 6-DoF grasping offers greater dexterity, suitable for complex scenarios. However, accurate generation of 6-DoF grasps necessitates geometric information, leading many existing methods to rely on 3D point cloud data. Despite significant progress achieved by grasp generation methods using point clouds, challenges persist due to measurement noise, occlusions, and environmental interference, making generating feasible and reliable grasps in cluttered scenes difficult. Additionally, many methods require time-consuming multi-stage processing for sampling grasp candidates and evaluating grasp quality, while the unavailability of 3D point cloud data in numerous applications exacerbates this issue \cite{ten2017grasp, liang2019pointnetgpd, du2021vision}. In contrast to 3D point clouds, acquiring color (RGB) images is more cost-effective and straightforward. With the advancements in deep learning, tasks such as object detection or 6D object pose estimation from RGB images have exhibited remarkable performance. However, the domain of grasp detection from RGB images remains largely unexplored.

In this work, we present a deep learning approach for grasp generation, focusing exclusively on leveraging RGB images to achieve accurate grasp estimation, building upon our previous research efforts \cite{hoang2023grasp, hoang2022context}. To obtain crucial geometric information for prediction, we employ recent advancements in monocular depth estimation to extract 3D points. Using an RGB image and the predicted 3D point cloud, we introduce an adaptive fusion module to extract discriminative features. These features are then input into a deep Hough voting module, inspired by our prior works \cite{hoang2023grasp, hoang2022context}, which has demonstrated the effectiveness of voting mechanisms in addressing occlusions and ensuring collision-free grasps. Following the voting module, the collected votes undergo clustering and regression processes to precisely determine the essential grasp parameters. We evaluate the proposed method on a standard dataset and in a real-world robot grasping application. The results demonstrate promising outcomes, indicating that even with the utilization of only RGB images and estimated depth maps, we achieve noteworthy results.