\section{Result and Discussion}
\label{sec:evaluation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[h!]
  \centering
  
  \begin{subfigure}[b]{0.98\linewidth}
    \includegraphics[width=0.48\linewidth]{figs/18_0}
    \includegraphics[width=0.48\linewidth]{figs/37_0}
  \caption{Scenes}  
  \end{subfigure}
 
  \begin{subfigure}[b]{0.98\linewidth}
    \includegraphics[width=0.48\linewidth]{figs/18_0_grasp}
    \includegraphics[width=0.48\linewidth]{figs/37_0_grasp}
  \caption{Predicted Grasps}  
  \end{subfigure}  
    
  \caption{Examples of input scenes and predicted grasps from the proposed method. The different intensity of grasp color denotes the confidence score of grasps. Green refers to the highest quality grasps and red refers to the lowest ones.}
  \label{fig:grasp_result}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Implementation Details}
\label{sec:implement}


In our implementation, we employ a pre-trained ResNet34 model trained on the ImageNet dataset as the encoder for RGB images. The output appearance feature from this encoder-decoder architecture comprises 256 channels. For point cloud feature extraction, we randomly sample 12,288 points from depth images and utilize a PointNet++ \cite{qi2017pointnet++}-based feature learning network, which also yields a 256-channel output. In the voting and context learning modules, we form $K=128$ clusters and produce a new feature map $\mathcal{F}_{context} \in 128 \times 512$. Subsequently, 128 grasps are generated from this new feature map. The prediction layer comprises $5 + V + 2A$ channels, with $V=120$ and $A=6$. We set $\lambda_{1}=\lambda_{2}=1.0$ and $\alpha$=$\beta$=$\gamma$=1.0. Our network is trained entirely using a batch size of 8 and optimized with Adam, employing a learning rate of 0.001 for 200 epochs. Training on a single Nvidia GeForce RTX 2080 Ti 11GB GPU takes approximately 20 hours. Regarding inference, our method requires 90ms for a single scene during the forward pass.

\subsection{Evaluation on GraspNet-1Billion}

We follow previous research \cite{fang2020graspnet} and evaluate our results on the dataset using $Precision@k$. This metric quantifies the precision of the top-k ranked grasps. To identify a predicted grasp ($G_{p}$) as a true positive, it must satisfy three conditions: (i) containing an object inside the gripper; (ii) being collision-free; (iii) exhibiting an antipodal grasp under a given friction coefficient $\mu$. The third condition is calculated based on prior works \cite{ten2017grasp, fang2020graspnet}. We denote $AP_{\mu}$ as the average $Precision@k$ for $k$ values ranging from 1 to 50, given a friction coefficient $\mu$. Additionally, we present the average of $AP_{\mu}$ across $\mu = \left\lbrace 0.2,0.4,0.6,0.8,1.0 \right\rbrace $, denoted as $AP$.


\begin{table*}[h]
\caption{The table shows the results on GraspNet-1Billion test set captured by RealSense/Kinect sensors respectively.}
\label{tab:grasp_detect_eval}
\begin{center}
\begin{tabular}{|l|c|c|c|c|c|c|c|c|c|}
\hline
& \multicolumn{3}{c|}{Seen} & \multicolumn{3}{c|}{Unseen (but similar)} & \multicolumn{3}{c|}{Novel} \\
\hline
& $AP$ & $AP_{0.8}$ & $AP_{0.4}$ & $AP$ & $AP_{0.8}$ & $AP_{0.4}$ & $AP$ & $AP_{0.8}$ & $AP_{0.4}$  \\
\hline
GG-CNN \cite{morrison2018closing} & 15.5/16.9 & 21.8/22.5 & 10.3/11.2 & 13.3/15.1 & 18.4/19.8 & 4.6/6.2 & 5.5/7.4 & 5.9/8.8 & 1.9/1.3 \\
\hline
Chu et al. \cite{chu2018real} & 16.0/17.6 & 23.7/24.7 & 10.8/12.7 & 15.4/17.4 & 20.2/21.6 & 7.1/8.9 & 7.6/8.0 & 8.7/9.3 & 2.5/1.8 \\
\hline
GPD \cite{ten2017grasp} & 22.9/24.4 & 28.5/30.2 & 12.8/13.5 & 21.3/23.2 & 27.8/28.6 & 9.6/11.3 & 8.2/9.6 & 8.9/10.1 & 2.7/3.2 \\
\hline
PN-GDP \cite{liang2019pointnetgpd} & 26.0/27.6 & 33.0/34.2 & 15.4/17.8 & 22.7/24.4 & 29.2/30.8 & 10.8/12.8 & 9.2/10.7 & 9.9/11.2 & 2.7/3.2 \\
\hline
Fang et al. \cite{fang2020graspnet} & 27.6/29.9 & 33.4/36.2 & 17.0/19.3 & 26.1/27.8 & 34.2/33.2 & 14.2/16.6 & 10.6/11.5 & 11.3/12.9 & 4.0/3.6 \\
\hline
Gou et al. \cite{gou2021rgb} & 28.0/32.1 & 33.5/39.5 & 17.8/20.9 & 27.2/30.4 & 36.3/37.9 & 15.6/18.7 & 12.3/13.1 & 12.5/13.8 & 5.6/6.0 \\
\hline
Contact \cite{sundermeyer2021contact} & 29.9/31.4 & 35.2/39.0 & 19.5/21.6 & 28.2/29.0 & 37.0/35.2 & 16.3/18.9 & 13.2/13.9 & 13.5/14.7 & 6.8/7.7 \\
\hline
VoteGrasp \cite{hoang2022context}  & 34.1/37.5 & 38.9/45.6 & 24.0/27.7 & 33.0/35.9 & 40.8/43.3 & 20.5/24.7 & 16.9/18.5 & 17.0/18.5 & 10.0/10.6 \\
\hline
Ours & \textbf{34.9}/\textbf{37.9} & \textbf{39.5}/\textbf{46.1} & \textbf{24.3}/\textbf{28.8} & \textbf{33.2}/\textbf{36.0} & \textbf{41.1}/\textbf{44.2} & \textbf{26.1}/\textbf{25.1} & \textbf{17.5}/\textbf{25.2} & \textbf{17.5}/\textbf{18.9} & \textbf{10.6}/\textbf{11.2} \\
\hline
\end{tabular}
\end{center}
\end{table*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Table \ref{tab:grasp_detect_eval} and Fig. \ref{fig:grasp_result} demonstrate the performance comparison between our approach and state-of-the-art methods. The evaluation utilized the evaluation metric adopted in \cite{fang2020graspnet}, enabling a direct comparison with related works reported in \cite{fang2020graspnet, gou2021rgb}. The table showcases the evaluation outcomes categorized into "Seen," "Unseen (but similar)," and "Novel" objects, aiding in assessing the model's generalization capability. The results indicate superior performance on scenes featuring seen objects across all methods, while notably, our proposed approach consistently outperforms others, even in the challenging "Novel" category, underscoring its robust generalization capabilities.

\textbf{Computational Time.} Our experiments were conducted on an Intel Xeon E-2716G CPU clocked at 3.7 GHz, paired with an Nvidia GeForce RTX 2080 Ti GPU featuring 11GB of memory. Our approach achieves a runtime of 100 ms per RGBD image. This fine balance between accuracy and speed empowers our method to proficiently generate grasp configurations in cluttered scenes, rendering it well-suited for diverse real-world scenarios.

\subsection{Robotic Grasping Experiment}
\label{sec:real_grasping}

The experiments were conducted with a Franka Emika Panda robot arm with 7-DOF, equipped with a parallel-jaw gripper. To capture RGBD data, we used either ASUS Xtion PRO LIVE sensor or  Microsoft Kinect sensor v2. The whole system is implemented using the ROS and MoveIt! frameworks. \\

\begin{table}[h!]
\caption{Results of real robot experiments. The networks were trained on the GraspNet-1Billion dataset. The table shows the number of attempts, the number of successful attempts, and the grasp success rate.}
\label{tab:real_grasping}
\begin{center}
\begin{tabular}{|l|c|c|c|}
\hline
Method & Attempt & Success & Success Rate \\
\hline
GPD \cite{ten2017grasp} & 300 & 195 & 65\% \\
PointNetGPD \cite{liang2019pointnetgpd} & 300 & 201 & 67\% \\
Fang et al. \cite{fang2020graspnet} & 300 & 214 & 71\% \\
Gou et al. \cite{gou2021rgb} & 300 & 218 & 73\% \\
Contact-GraspNet \cite{sundermeyer2021contact}  & 300 & 222 & 74\% \\
VoteGrasp \cite{hoang2022context} & 300 & 234 & 78\% \\
Ours & 300 & \textbf{240} & \textbf{80}\% \\
\hline
\end{tabular}
\end{center}
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We conducted a real-world evaluation of state-of-the-art grasp detection methods, each trained on the GraspNet-1Billion dataset for a fair comparison. We selected novel objects tailored to fit the gripper's shapes and sizes. In each scenario, a random subset of 10-15 objects was arranged in a haphazard manner on a table, mirroring the unpredictability of real-world environments. Each method underwent 300 grasp attempts, with the robot randomly selecting objects. A grasp was deemed successful if the robot could grasp and lift the object within a single attempt. The results in Table \ref{tab:real_grasping} demonstrate our method's superiority, achieving an 84\% success rate outperforming all other methods. This highlights the proposed framework's efficacy in real-world grasping scenarios, attributing the increased success rate to the integration of estimated depth data, underscoring the significance of richer input data for precise and effective.  \\

\section{Conclusions}

In this study, we addressed the fundamental challenge of grasp generation in robotic manipulation by introducing an innovative approach that bypasses the need for specialized depth sensors. Our method revolutionizes grasp generation by leveraging tailored deep learning techniques to estimate depth from color (RGB) images directly. This paradigm shift allows the computation of predicted point clouds solely from RGB inputs, eliminating the dependency on traditional depth sensors. A pivotal contribution lies in the development of a fusion module adept at seamlessly integrating features derived from RGB images with those inferred from predicted point clouds. This fusion process harnesses the strengths of both modalities, significantly enhancing grasp configurations. Experimental evaluations unequivocally validate the effectiveness of our approach, demonstrating its superiority in generating grasp configurations compared to existing methods. Future endeavors in these outlined directions hold the promise of further enhancing the versatility, adaptability, and real-world applicability of grasp generation in robotics.