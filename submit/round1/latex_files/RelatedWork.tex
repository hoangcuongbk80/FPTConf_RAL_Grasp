\section{Literature Review}
\label{sec:relatedwork}

In this section, we review relevant works, specifically focus-
ing on existing learning-based grasp generation methods and datasets for monocular depth estimation.

\subsection{Learning-based Grasp Generation}

The grasp pose detection problem involves predicting multiple poses within a scene, enabling robots to manipulate objects effectively. Earlier approaches \cite{bicchi2000robotic, miller2003automatic} assumed complete 2D or 3D object knowledge or simplified objects as primitive shapes, facing limitations in obtaining accurate 3D models. Learning-based methods emerged, utilizing large-scale data and automated feature extraction. Some focused on 4-DoF grasp poses on the camera plane, known as "top-down grasping," restricting degrees of freedom and potentially missing crucial grasp poses, like those along object edges. In contrast, 6-DoF grasp poses offer increased flexibility and complexity, allowing grasping from various directions, necessitating six parameters to define location and rotation, with potential inclusion of additional degrees of freedom, like gripper width or height. Learning-based grasp generation can be categorized into two primary algorithmic methodologies for grasp synthesis: grasp pose sampling and and regressing grasp pose directly. Sampling-based approaches, like GPD \cite{ten2017grasp} and PointNetGPD \cite{liang2019pointnetgpd}, evaluate individual grasp samples. Despite dense sampling, they struggle in regions like the rims of objects where surface normals estimation is unreliable. Some methods, such as Lou et al. \cite{lou2020learning}, sample wrist angles independently, while others, like Kokic et al. \cite{kokic2020learning}, sample grasp, roll angles, and offset distances. However, these approaches often trade computation time for generated grasp poses, resulting in limited poses per scene and a focus on local object features. Direct regression methods, exemplified by Schmidt et al. \cite{schmidt2018grasping} and Yang et al. \cite{yang2021robotic}, predict grasp poses or transformation matrices directly from visual data, processing information holistically. Yet, approaches like GraspNet \cite{fang2020graspnet} and PointNet++ \cite{ni2020pointnet++}, utilizing entire scene point clouds, lack consideration for inter-object relationships, limiting performance in cluttered scenes and under occlusion. To overcome these limitations, our previous work \cite{hoang2023grasp, hoang2022context} leverage a voting mechanism and contextual information to directly generate grasp configurations from 3D point clouds, addressing challenges in occlusion common in manipulationThe proposed method aligns closely with our prior studies \cite{hoang2023grasp, hoang2022context}. However, rather than relying on 3D data from depth sensors, we explore the utilization of depth images estimated via a monocular depth estimation framework. 

\subsection{Monocular Depth Estimation}

The inception of monocular depth estimation was pioneered by \cite{saxena2005learning, saxena2008make3d}, employing hand-engineered features and Markov Random Fields (MRF). Subsequently, the advent of deep learning, spearheaded by Eigen et al. \cite{eigen2014depth}, revolutionized depth estimation. However, learned depth regression encounters challenges in the decoder phase due to the loss of fine details from successive convolution layers in neural networks. Numerous approaches have addressed this issue diversely. \cite{eigen2015predicting} introduced multi-scale networks to predict depth at multiple resolutions. Laina et al. \cite{laina2016deeper} enhanced a ResNet architecture with improved up-sampling blocks to mitigate information loss. Xu et al. \cite{xu2017multi} combined deep learning with conditional random fields (CRF) for feature fusion at different scales. Another line of research pursued multitask learning, simultaneously predicting semantic labels \cite{jiao2018look}, depth edges, and normals \cite{ramamonjisoa2019sharpnet, zhang2019pattern, lee2019big} to refine depth predictions. Kendall et al. \cite{kendall2018multi} explored uncertainty estimation's impact on scene understanding, while Yin et al. \cite{yin2019enforcing} used surface geometry to estimate 3D point clouds from predicted depth maps. Recent works by Bhat et al. propose a classification-based formulation for distance prediction [29]. Tian et al. \cite{chen2020improving} integrated attention blocks into the decoder, while Transformer-based architectures gained traction \cite{ranftl2021vision, yang2021transformer}.